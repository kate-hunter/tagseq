#From https://wiki.chpc.utah.edu/display/~u0424091/Snakemake
#dry run: snakemake -n -s snakefile

#rule stock:
#    input:
#    output:
#    resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
#    params:cpu=1
#    shell:"""
#

###USER INPUT####
project_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/"
reads_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/fastq_files/GSAF_files_prac/"
original_files="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/fastq_files/GSAF_files_prac/original_files/"
#snake_scripts_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/snake/khprac/"
genome="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/"
fasta_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/genome_files/NMEL_genome_v2.1.0.fasta"
gff_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/genome_files/NMEL_OGS_v2.1.0.gff3"
rscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/snake/khprac/Rstuff/"
plscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/TagSeq_processing_code/"
strandedness="unstranded"
#need to change line 143, and 182 if I change the strandedness
#discovered that strandedness needs to be no because this is single-end sequencing
#####End INPUT###


ids, =glob_wildcards(original_files+"{id}_L001_R1_001.fastq.gz")

rule all:
        input:expand(original_files+"{sample}_L001_R1_001.fastq.gz", sample=ids),
              expand(reads_path+"concatenated/"+"{sample}.cmb.fastq.gz", sample=ids),
              expand(reads_path+"trimmed/"+"{sample}.trim.fastq", sample=ids),
              expand(reads_path+"qual_filter/"+"{sample}.qualfilt", sample=ids)
              #genome+"index_done.txt",
              #expand(reads_path+"SAM/KHfiles/{sample}/"+"{sample}_Aligned.out.sam", sample=ids)
              #expand(reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam", sample=ids),
              #expand(reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam", sample=ids),
              #expand(reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt", sample=ids),
              #reads_path+"htseq/strandedis"+strandedness+"/matrix.csv",
              #expand(reads_path+"featurecounts/strandedis"+strandedness+"/{sample}.txt", sample=ids),
              #expand(reads_path+"outputsfolder/strandedis"+strandedness+"/{sample}", sample=ids),
              #expand(reads_path+"outputsfolder/strandedis"+strandedness+"/{sample}.summary", sample=ids)

# Say you have a .fastq.gz file named <sample_name>.fastq.gz for each sample
# in the list ids. This rule will run unzip_files for
# each of those .fastq.gz files:

rule combine:
        input: input1=original_files+"{sample}_L001_R1_001.fastq.gz", 
               input2=original_files+"{sample}_L002_R1_001.fastq.gz"
        output: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=2,
                id="{sample}",
                input_path=original_files
        shell:"""
              cd {params.input_path}
              zcat {input.input1} {input.input2} | gzip -c >{output}
              """

#below rule checks for duplicates as well as only keeps sequences with the right leader sequence (then trims the leader)
rule clip:
       input: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
       output: reads_path+"trimmed/"+"{sample}.trim.fastq"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"concatenated/",
              plscript=plscripts+"tagseq_clipper.anna.pl",
              trim=reads_path+"trimmed/"
       shell:"""
             cd {params.input_path}
             chmod +x {params.plscript}
             perl {params.plscript} {input} '' 0 24 > {output}
             """

#the below rule quality filters as well as clips adaptors off
rule fastxtools:
       input: reads_path+"trimmed/"+"{sample}.trim.fastq"
       output: reads_path+"qual_filter/"+"{sample}.qualfilt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"trimmed/"
       shell:"""
             cd {params.input_path}
             module load fastx_toolkit
             fastx_clipper -a AAAAAAAA -l 20 -Q33 -i {input} | fastx_clipper -a AGATCGGAAG -l 20 -Q33 | fastq_quality_filter -Q33 -q 20 -p 90 > {output}
             """

#| fastx_clipper -a AGATCGGAAG -l 20 -Q33 | fastq_quality_filter -Q33 -q 10 -p 90 >
# The sjdbOverhang is 99 here because some seem to just stick with the defaults
# the attempt change to 40,0000 x attempt, because by default STAR uses it's 31G
# Based on Tim's advice, upping CPUs to 6 and upping the threads (where every CPU=2threads/cores)
rule genome_index:
        input: fasta_file
        output: genome+"index_done.txt"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                fasta=fasta_file,
                gff=gff_file,
                output_indexdirec=genome+"indexing"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode genomeGenerate \
              --genomeDir {params.output_indexdirec} \
              --genomeFastaFiles {params.fasta} \
              --sjdbGTFfile {params.gff} \
              --sjdbGTFtagExonParentTranscript Parent \
              --sjdbOverhang 99
              touch {output}
              """

rule align_reads_SAM:
        input: reads_path+"qual_filter/"+"{sample}.qualfilt"
        output: reads_path+"SAM/KHfiles/{sample}/"+"{sample}_Aligned.out.sam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                id="{sample}",
                indexdirec=genome+"indexing",
                folder_loca=reads_path+"SAM/KHfiles/{sample}/{sample}_"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode alignReads \
              --genomeDir {params.indexdirec} \
              --outSAMtype SAM \
              --readFilesIn {input} \
              --outFileNamePrefix {params.folder_loca}
              """


rule convertSAMtoBAMsort:
        input: reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam"
        output: bam=reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam",
                sorted=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=2
        shell:"""
              module load samtools/1.5
              samtools view -S -b {input}>{output.bam}
              samtools sort {output.bam} -o {output.sorted} 
              """

#Need a rule to combine leftovers into a file here 


# rule qualimap:
#         input:
#         output:
#         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
#         params: cpu=1
#         shell:"""
#               """

#the parameters for the counting rule
# -s should be yes (default, but I included it here) because Tagseq is strand-specific
# -m should be union because the read can then overlap more than one feature
# -t exon because the utr is getting transcribed.
# -i Parent because that's what the the attribute is called in gff
#https://www.embopress.org/doi/full/10.15252/msb.20209539
# above is the code that helped me decide what to do in relation to htseq

rule counting:
        input: reads_path+"BAM/{sample}/sortedbyname/{sample}namesort.bam"
        output: reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=1,
                gff=gff_file,
                id="{sample}",
                strands="no"
        shell:"""
              module load python/3.7.3
              python -m HTSeq.scripts.count \
              -f bam\
              -m union \
              -r name \
              -s {params.strands}\
              -t mRNA \
              -i Parent \
              {input} {params.gff} > {output}
              """

rule combine_matrices_htseq:
        input: reads_path+"htseq/strandedis"+strandedness+"/"
        output: reads_path+"htseq/strandedis"+strandedness+"/matrix.csv"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=1,
                concat_script=rscripts+"htseq_scriptconcat.R"
        shell:"""
              cd {input}
              module load R
              Rscript {params.concat_script}
              """


####featurecounts to run in order to see whether there were more matches with feature counts than with htseq
        #        featureCounts  -F 'GTF' \  ##for gff file
        #        -a {input.gff} \ ### input gff file
        #        -g 'ID'\  ##for the gene ID
        #        -s 2 \ ##for reverse
        #        -t 'gene'\  ##for what kind of ID
        #        -O \ ###multi-overlap allowed

rule featurecounts:
        input: gff=gff_file, 
               bam=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        output: reads_path+"featurecounts/strandedis"+strandedness+"/{sample}.txt"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=1,
                strands='0'
        shell: """
               module load subread/1.5.3
               featureCounts \
               -a {input.gff} \
               -g 'ID'\
               -s {params.strands} \
               -t 'gene'\
               -O \
               -o {output} \
               {input.bam}
               """

rule combine_outputs:
        input: samlogs=reads_path+"SAM/{sample}/{sample}_Log.final.out",
               htseq_logs=reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt",
               featurecounts_logs=reads_path+"featurecounts/strandedis"+strandedness+"/{sample}.txt.summary"
        output: out=reads_path+"outputsfolder/strandedis"+strandedness+"/{sample}",
                temp=temp("{sample}.temp"),
                temp1=temp("{sample}.temp1"),
                temp2=temp("{sample}.temp2"),
                temp3=temp("{sample}.temp3"),
                temp4=temp("{sample}.temp4")
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=2,
                awks='{Total=Total+$2} END{print "Total is: " Total}'
        shell:"""
              tail -n 5 {input.htseq_logs} > {output.temp}
              sed -e '1i\#############HTSEQ_data################' {output.temp} > {output.temp2}
              sed -e '1i\#############SAM.log_data##############' {input.samlogs} > {output.temp1}
              sed -e '1i\#############featurecounts.log##############' {input.featurecounts_logs} > {output.temp3}
              head -n -5 {input.htseq_logs} | awk '{params.awks}' > {output.temp4}
              cat {output.temp1} {output.temp2} {output.temp4} {output.temp3} > {output.out}
              """
#Now I want to make a file that will give me the changes in line number between each file
#Number of reads mapped with SAM
#Number of reads 

rule read_number:
        input: reads_path+"outputsfolder/strandedis"+strandedness+"/{sample}"
        output: reads_path+"outputsfolder/strandedis"+strandedness+"/{sample}.summary"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=2    
        shell:"""
              sed -n '1p;7p;10p;25p;32p;39p;40p;44p;45p;46p;48p;54p;57p' {input}>{output}
              """

#Alright, I think I'm finally onto the next step after featurecounts!!!