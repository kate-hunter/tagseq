#From https://wiki.chpc.utah.edu/display/~u0424091/Snakemake
#dry run: snakemake -n -s snakefile

#rule stock:
#    input:
#    output:
#    resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
#    params:cpu=1
#    shell:"""
#

###USER INPUT####
project_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/"
reads_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/fastq_files/GSAF_files_prac/"
original_files="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/fastq_files/GSAF_files_prac/original_files/"
snake_scripts_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/snake/khprac/"
genome="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/"
fasta_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/genome_files/NMEL_genome_v2.1.0.fasta"
gff_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/genome/genome_files/NMEL_OGS_v2.1.0.gff3"
rscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/snake/khprac/Rstuff/"
plscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/perl_scripts/"
bashscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/bash_scripts/"
awkscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/nmel_immune_tagseq/kate_practice/nmel_tagseq/scripts/awk_scripts/"
#####End INPUT###


ids, =glob_wildcards(original_files+"{id}_L001_R1_001.fastq.gz")

rule all:
        input:expand(original_files+"{sample}_L001_R1_001.fastq.gz", sample=ids),
              #expand(reads_path+"concatenated/"+"{sample}.cmb.fastq.gz", sample=ids),
              #expand(reads_path+"trimmed/"+"{sample}.trim.fastq", sample=ids),
              #expand(reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt", sample=ids)
              #genome+"index_done.txt",
              #expand(reads_path+"SAM/KHfiles/{sample}/"+"{sample}_Aligned.out.sam", sample=ids)
              #expand(reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam", sample=ids),
              #expand(reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam", sample=ids),
              #expand(reads_path+"featurecounts/"+"{sample}.txt", sample=ids),
              reads_path+"outputsfolder/seqcounts.txt", 
              reads_path+"outputsfolder/concat.txt",
              reads_path+"outputsfolder/cmb_orig.txt", 
              reads_path+"outputsfolder/trim.txt",
              reads_path+"outputsfolder/qual.txt",
              reads_path+"outputsfolder/joined_tables.txt", 
              reads_path+"outputsfolder/joined_long.txt"
              #expand(reads_path+"outputsfolder/"+"{sample}.summary", sample=ids)

#Some rules I took out below
              #expand(reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt", sample=ids),
              #reads_path+"htseq/strandedis"+strandedness+"/matrix.csv",

# Say you have a .fastq.gz file named <sample_name>.fastq.gz for each sample
# in the list ids. This rule will run unzip_files for
# each of those .fastq.gz files:

rule combine:
        input: input1=original_files+"{sample}_L001_R1_001.fastq.gz", 
               input2=original_files+"{sample}_L002_R1_001.fastq.gz"
        output: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=2,
                id="{sample}",
                input_path=original_files
        shell:"""
              cd {params.input_path}
              zcat {input.input1} {input.input2} | gzip -c >{output}
              """

#below rule checks for duplicates as well as only keeps sequences with the right leader sequence (then trims the leader)
rule clip:
       input: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
       output: reads_path+"trimmed/"+"{sample}.trim.fastq"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"concatenated/",
              plscript=plscripts+"tagseq_clipper.pl",
              trim=reads_path+"trimmed/"
       shell:"""
             cd {params.input_path}
             chmod +x {params.plscript}
             perl {params.plscript} {input} '' 0 24 > {output}
             """

#the below rule quality filters as well as clips adaptors off
rule cutadapt:
       input: reads_path+"trimmed/"+"{sample}.trim.fastq"
       output: reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"trimmed/"
       shell:"""
             cd {params.input_path}
             module load fastx_toolkit
             module load cutadapt
             cutadapt -a AAAAAAAAA -m 20 -O 4 -q 20 {input} | \
             cutadapt -a AGATCGGAAG -m 20 -O 7 -q 20 - | \
             fastq_quality_filter -Q33 -q 10 -p 90 > {output}
             """

#The above will cut either the poly-A adaptor or the other adaptor, will remove anything that ends up being less than 20 bases (-m)
#and will trim low-quality ends from reads at the 10. I decided to keep using the fastq_quality_filter because I couldn't tell whehter cutadapt would do this

#| fastq_quality_filter -Q33 -q 10 -p 90

#| fastx_clipper -a AGATCGGAAG -l 20 -Q33 | fastq_quality_filter -Q33 -q 10 -p 90 >
# The sjdbOverhang is 99 here because some seem to just stick with the defaults
# the attempt change to 40,0000 x attempt, because by default STAR uses it's 31G
# Based on Tim's advice, upping CPUs to 6 and upping the threads (where every CPU=2threads/cores)
rule genome_index:
        input: fasta_file
        output: genome+"index_done.txt"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                fasta=fasta_file,
                gff=gff_file,
                output_indexdirec=genome+"indexing"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode genomeGenerate \
              --genomeDir {params.output_indexdirec} \
              --genomeFastaFiles {params.fasta} \
              --sjdbGTFfile {params.gff} \
              --sjdbGTFtagExonParentTranscript Parent \
              --sjdbOverhang 99
              touch {output}
              """

rule align_reads_SAM:
        input: reads_path+"qual_filter/"+"{sample}.qualfilt"
        output: reads_path+"SAM/KHfiles/{sample}/"+"{sample}_Aligned.out.sam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                id="{sample}",
                indexdirec=genome+"indexing",
                folder_loca=reads_path+"SAM/KHfiles/{sample}/{sample}_"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode alignReads \
              --genomeDir {params.indexdirec} \
              --outSAMtype SAM \
              --readFilesIn {input} \
              --outFileNamePrefix {params.folder_loca}
              """


rule convertSAMtoBAMsort:
        input: reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam"
        output: bam=reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam",
                sorted=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=2
        shell:"""
              module load samtools/1.5
              samtools view -S -b {input}>{output.bam}
              samtools sort {output.bam} -o {output.sorted} 
              """

# rule qualimap:
        #         input:
        #         output:
        #         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        #         params: cpu=1
        #         shell:"""
        #               """

##### Here I chose to use featurecounts because it was recommended at UT Austin, but I'm keeping the HTseq line in in case I want to go back. 
        #the parameters for the counting rule
        # -s should be no (default is yes) 
        #discovered that strandedness needs to be no because this is single-end sequencing
        # -m should be union because the read can then overlap more than one feature
        # -t exon because the utr is getting transcribed.
        # -i Parent because that's what the the attribute is called in gff
        #https://www.embopress.org/doi/full/10.15252/msb.20209539
        # above is the code that helped me decide what to do in relation to htseq

# rule counting:
        #         input: reads_path+"BAM/{sample}/sortedbyname/{sample}namesort.bam"
        #         output: reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt"
        #         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        #         params: cpu=1,
        #                 gff=gff_file,
        #                 id="{sample}",
        #                 strands="no"
        #         shell:"""
        #               module load python/3.7.3
        #               python -m HTSeq.scripts.count \
        #               -f bam\
        #               -m union \
        #               -r name \
        #               -s {params.strands}\
        #               -t mRNA \
        #               -i Parent \
        #               {input} {params.gff} > {output}
        #               """

####rule combine_matrices_htseq:
        #         input: reads_path+"htseq/"
        #         output: reads_path+"htseq/"+"/matrix.csv"
        #         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        #         params: cpu=1,
        #                 concat_script=rscripts+"htseq_scriptconcat.R"
        #         shell:"""
        #               cd {input}
        #               module load R
        #               Rscript {params.concat_script}
        #               """

####featurecounts to run in order to see whether there were more matches with feature counts than with htseq
        #        featureCounts  -F 'GTF' \  ##for gff file
        #        -a {input.gff} \ ### input gff file
        #        -g 'ID'\  ##for the gene ID
        #        -s 1 \ ##because we have a sense-strand assay (positive)
        #        -t 'gene'\  ##for what kind of ID
        #        -O \ ###multi-overlap allowed

rule featurecounts:
        input: gff=gff_file, 
               bam=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        output: reads_path+"featurecounts/"+"{sample}.txt"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=1,
                strands='1'
        shell: """
               module load subread/1.5.3
               featureCounts \
               -a {input.gff} \
               -g 'ID'\
               -s {params.strands} \
               -t 'gene'\
               -O \
               -o {output} \
               {input.bam}
               """


###### Want to make a rule where I collect the counted outputs from each step
#Ok, instead, I'm just going to run this as a separate bash script. 
#Go to the tabulating.sh for this! 

rule summaryoutput:
       input: origfiles=original_files,
              concatenated=reads_path+"concatenated/",
              trimmed=reads_path+"trimmed/", 
              qualfilt=reads_path+"qual_filter/cutadapt/"
       output: orig=reads_path+"outputsfolder/seqcounts.txt",
               concat=reads_path+"outputsfolder/concat.txt",
               trim=reads_path+"outputsfolder/trim.txt",
               qual=reads_path+"outputsfolder/qual.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              bashscript_tab=bashscripts+"tabulating.sh", 
              bashscript_nonzip=bashscripts+"summing_seq.sh"
       shell:"""
             sh {params.bashscript_tab} {input.origfiles} originalseqs_totals {output.orig} 
             sh {params.bashscript_tab} {input.concatenated} concatenated_totals {output.concat}
             sh {params.bashscript_nonzip} {input.trimmed} trimmed_totals {output.trim}
             sh {params.bashscript_nonzip} {input.qualfilt} quality_totals {output.qual}
             """
             
rule cmboutput:
       input: orig=reads_path+"outputsfolder/seqcounts.txt"
       output: cmb_orig=reads_path+"outputsfolder/cmb_orig.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              bashscript_comb=bashscripts+"combining.sh"
       shell:"""
             sh {params.bashscript_comb} {input.orig} {output.cmb_orig}
             """

#below is the rule to join all the tables together by their sample name and then to transpose join command will need to be adjusted based on the number of tables that go into this
rule joined_tables:
       input: first=reads_path+"outputsfolder/cmb_orig.txt",
              sectolast=reads_path+"outputsfolder/concat.txt",
              last=reads_path+"outputsfolder/trim.txt"
       output: tall_table=reads_path+"outputsfolder/joined_tables.txt",
               long_table=reads_path+"outputsfolder/joined_long.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4, 
              awk_script=awkscripts+"tst.awk"
       shell:"""
             join --header {input.sectolast} {input.last} | join --header {input.first} - \
             | column -t > {output.tall_table}
             awk -f {params.awk_script} {output.tall_table} | column -t > {output.long_table}
             """
        
