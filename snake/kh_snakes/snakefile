 #From https://wiki.chpc.utah.edu/display/~u0424091/Snakemake
#dry run: snakemake -n -s snakefile
#if you are getting that all the code wants to run several times over even though you already ran it, you can use this 
#snakemake --touch --cores


#rule stock:
#    input:
#    output:
#    resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
#    params:cpu=1
#    shell:"""
#

###USER INPUT####G
reads_path="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/analysis/GSAF_files_prac/"
original_files="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/analysis/GSAF_files_prac/original_files/"
genome="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/genome/"
fasta_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/genome/genome_files/NMEL_genome_v2.1.0.fasta"
gff_file="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/genome/genome_files/NMEL_OGS_v2.1.0.gff3"
plscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/scripts/perl_scripts/"
bashscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/scripts/bash_scripts/"
awkscripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/scripts/awk_scripts/"
R_scripts="/uufs/chpc.utah.edu/common/home/kapheim-group2/kate_projects/nmel_tagseq/scripts/R_scripts/"
#####End INPUT###


ids, =glob_wildcards(original_files+"{id}_L001_R1_001.fastq.gz")

rule all:
        input:genome+"index_done.txt",
              expand(original_files+"{sample}_L001_R1_001.fastq.gz", sample=ids),
              expand(reads_path+"concatenated/"+"{sample}.cmb.fastq.gz", sample=ids),
              expand(reads_path+"trimmed/"+"{sample}.trim.fastq", sample=ids),
              expand(reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt", sample=ids),
              expand(reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam", sample=ids),
              expand(reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam", sample=ids),
              expand(reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam", sample=ids),
              expand(reads_path+"featurecounts/"+"{sample}.txt", sample=ids),
              reads_path+"outputsfolder/seqcounts.txt",
              reads_path+"outputsfolder/concat.txt",
              reads_path+"outputsfolder/cmb_orig.txt", 
              reads_path+"outputsfolder/trim.txt",
              reads_path+"outputsfolder/qual.txt",
              reads_path+"outputsfolder/align.txt",
              reads_path+"outputsfolder/featurecounts.txt",
              reads_path+"outputsfolder/joined_tables.txt", 
              reads_path+"outputsfolder/joined_long.txt",
              expand(reads_path+"trimmed/"+"{sample}.summ.txt", sample=ids)

# Say you have a .fastq.gz file named <sample_name>.fastq.gz for each sample
# in the list ids. This rule will run unzip_files for
# each of those .fastq.gz files:

rule combine:
        input: input1=original_files+"{sample}_L001_R1_001.fastq.gz", 
               input2=original_files+"{sample}_L002_R1_001.fastq.gz"
        output: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=2,
                id="{sample}",
                input_path=original_files
        shell:"""
              cd {params.input_path}
              zcat {input.input1} {input.input2} | gzip -c >{output}
              """

#below rule checks for duplicates as well as only keeps sequences with the right leader sequence (then trims the leader)
#this checks that the sequences have a leader (important to checking for duplicates)
#then for after the header-check for duplicates based on degernate header, Anna Battenhouse recommended I use 24 based on my histograms suggesting that this is long enough
#I could also generate a histogram to show how all the files stack up but here have not done so. 

#Next steps, I do need to get the 
rule clip:
       input: reads_path+"concatenated/"+"{sample}.cmb.fastq.gz"
       output: trim=reads_path+"trimmed/"+"{sample}.trim.fastq",
               outtrim=reads_path+"trimmed/"+"{sample}.cmb.fastq.gz.summ.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"concatenated/",
              plscript=plscripts+"tagseq_clipper.anna.pl",
              kh_pledit=plscripts+"tagseq_clipper.anna_khedited.pl",
              outtrim=reads_path+"concatenated/"+"{sample}.summ.txt"
       shell:"""
             cd {params.input_path}
             chmod +x {params.kh_pledit}
             perl {params.kh_pledit} {input} '' 0 24 > {output.trim} 
             cp {params.outtrim} > {output.outtrim}
             """

#the below rule quality filters as well as clips adaptors off, because the poly-a tail or adaptor might be there, we run through two rounds of cutadapt (this is taken from 
# tagseqtrim_launch script, which pipes outputs out)
rule cutadapt:
       input: reads_path+"trimmed/"+"{sample}.trim.fastq"
       output: reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              input_path=reads_path+"trimmed/"
       shell:"""
             cd {params.input_path}
             module load fastx_toolkit
             module load cutadapt
             cutadapt -a AAAAAAAAA -m 20 -O 4 -q 20 {input} | \
             cutadapt -a AGATCGGAAG -m 20 -O 7 -q 20 - | \
             fastq_quality_filter -Q33 -q 10 -p 90 > {output}
             """

#The above will cut either the poly-A adaptor or the other adaptor, will remove anything that ends up being less than 20 bases (-m)
#and will trim low-quality ends from reads at the 10. I decided to keep using the fastq_quality_filter because I couldn't tell whehter cutadapt would do this

#| fastq_quality_filter -Q33 -q 10 -p 90

#| fastx_clipper -a AGATCGGAAG -l 20 -Q33 | fastq_quality_filter -Q33 -q 10 -p 90 >
# The sjdbOverhang is 99 here because some seem to just stick with the defaults
# the attempt change to 40,0000 x attempt, because by default STAR uses it's 31G
# Based on Tim's advice, upping CPUs to 6 and upping the threads (where every CPU=2threads/cores)
rule genome_index:
        input: fasta_file
        output: genome+"index_done.txt"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                fasta=fasta_file,
                gff=gff_file,
                output_indexdirec=genome+"indexing"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode genomeGenerate \
              --genomeDir {params.output_indexdirec} \
              --genomeFastaFiles {params.fasta} \
              --sjdbGTFfile {params.gff} \
              --sjdbGTFtagExonParentTranscript Parent \
              --sjdbOverhang 99
              touch {output}
              """

rule align_reads_SAM:
        input: reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt"
        output: reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=6,
                id="{sample}",
                indexdirec=genome+"indexing",
                folder_loca=reads_path+"SAM/{sample}/{sample}_"
        shell:"""
              module load star
              STAR --runThreadN 12 \
              --runMode alignReads \
              --genomeDir {params.indexdirec} \
              --outSAMtype SAM \
              --readFilesIn {input} \
              --outFileNamePrefix {params.folder_loca}
              """


rule convertSAMtoBAMsort:
        input: reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam"
        output: bam=reads_path+"BAM/{sample}/"+"{sample}_Aligned.out.bam",
                sorted=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        resources: mem_mb=lambda wildcards,  attempt: (40000*attempt)
        params: cpu=2
        shell:"""
              module load samtools/1.5
              samtools view -S -b {input}>{output.bam}
              samtools sort {output.bam} -o {output.sorted} 
              """

##### Here I chose to use featurecounts because it was recommended at UT Austin, but I'm keeping the HTseq line in in case I want to go back. 
        #the parameters for the counting rule
        # -s should be no (default is yes) 
        #discovered that strandedness needs to be no because this is single-end sequencing
        # -m should be union because the read can then overlap more than one feature
        # -t exon because the utr is getting transcribed.
        # -i Parent because that's what the the attribute is called in gff
        #https://www.embopress.org/doi/full/10.15252/msb.20209539
        # above is the code that helped me decide what to do in relation to htseq


####featurecounts to run in order to see whether there were more matches with feature counts than with htseq
        #        featureCounts  -F 'GTF' \  ##for gff file
        #        -a {input.gff} \ ### input gff file
        #        -g 'ID'\  ##for the gene ID
        #        -s 1 \ ##because we have a sense-strand assay (positive)
        #        -t 'gene'\  ##for what kind of ID, this I validated is ok a few ways, one being here https://github.com/grovesdixon/Acropora_gene_expression_meta/blob/master/ge_meta_data_processing.txt from this paper: doi: 10.1111/mec.15535)
        #        -O \ ###multi-overlap allowed

rule featurecounts:
        input: gff=gff_file, 
               bam=reads_path+"BAM/{sample}/sortedbycoord/{sample}namesort.bam"
        output: reads_path+"featurecounts/"+"{sample}.txt"
        resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        params: cpu=1
        shell: """
               module load subread/1.5.3
               featureCounts \
               -a {input.gff} \
               -g 'ID'\
               -s 1 \
               -t 'gene'\
               -O \
               -o {output} \
               {input.bam}
               """


###### Want to make a rule where I collect the counted outputs from each step
rule summaryoutputs:
       input: orig=expand(original_files+"{sample}_L001_R1_001.fastq.gz", sample=ids),
              concat=expand(reads_path+"concatenated/"+"{sample}.cmb.fastq.gz", sample=ids),
              trim=expand(reads_path+"trimmed/"+"{sample}.trim.fastq", sample=ids),
              qual=expand(reads_path+"qual_filter/cutadapt/"+"{sample}.qualfilt", sample=ids),
              sam=expand(reads_path+"SAM/{sample}/"+"{sample}_Aligned.out.sam", sample=ids), 
              feats=expand(reads_path+"featurecounts/{sample}.txt.summary", sample=ids)
       output: orig=reads_path+"outputsfolder/seqcounts.txt",
               concat=reads_path+"outputsfolder/concat.txt",
               trim=reads_path+"outputsfolder/trim.txt",
               qual=reads_path+"outputsfolder/qual.txt",
               align=reads_path+"outputsfolder/align.txt",
               featurecounts=reads_path+"outputsfolder/featurecounts.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              origfiles=original_files,
              concatenated=reads_path+"concatenated/",
              trimmed=reads_path+"trimmed/", 
              qualfilt=reads_path+"qual_filter/cutadapt/",
              align=reads_path+"SAM/",
              featurecounts=reads_path+"featurecounts/",
              bashscript_tab=bashscripts+"summing_origconcatseqs.sh", 
              bashscript_nonzip=bashscripts+"summing_nonzip.sh", 
              bashscript_align=bashscripts+"summing_alignment.sh", 
              bashscript_featurecounts=bashscripts+"summing_featurecounts.sh"
       shell:"""
             sh {params.bashscript_tab} {params.origfiles} originalseqs_totals {output.orig}
             sh {params.bashscript_tab} {params.concatenated} concatenated_totals {output.concat} 
             sh {params.bashscript_nonzip} {params.trimmed} trimmed_totals {output.trim} .fastq
             sh {params.bashscript_nonzip} {params.qualfilt} qualcheck_totals {output.qual} .qualfilt
             sh {params.bashscript_align} {params.align} {output.align}
             sh {params.bashscript_featurecounts} {params.featurecounts} {output.featurecounts}
             """

#the below rule was made to combine the lanes from the original seq counts file so that that could be used in the next steps (as opposed to having two rows for each sample)

rule cmboutput:
       input: orig=reads_path+"outputsfolder/seqcounts.txt"
       output: cmb_orig=reads_path+"outputsfolder/cmb_orig.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4,
              bashscript_comb=bashscripts+"combining.sh"
       shell:"""
             sh {params.bashscript_comb} {input.orig} {output.cmb_orig}
             """

#I initially had figured out how to combine all of the tables together, but in hindsight was having trouble keeping all of the data together. 
#instead, I will now just process the next steps in R instead-I will keep script so I can run it again if needed.


rule joininginR:
       input: first=reads_path+"outputsfolder/cmb_orig.txt",
              sec=reads_path+"outputsfolder/concat.txt",
              third=reads_path+"outputsfolder/trim.txt",
              fourth=reads_path+"outputsfolder/qual.txt",
              sectolast=reads_path+"outputsfolder/align.txt",
              last=reads_path+"outputsfolder/featurecounts.txt"
       output: R_table=reads_path+"outputsfolder/Rcombtable.txt",
               tall_table=reads_path+"outputsfolder/joined_tables.txt",
               long_table=reads_path+"outputsfolder/joined_long.txt"
       resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
       params:cpu=4, 
              r_merge=R_scripts+"comb_tables.R",
              awk_script=awkscripts+"tst.awk",
              outdir=reads_path+"outputsfolder/",
              table_name="Rcombtable.txt"
       shell:"""
             cd {params.outdir}
             Rscript {params.r_merge} {input.first} {input.sec} {input.third} {input.fourth} {input.sectolast} {input.last} {params.table_name} 
             column -t {output.R_table}> {output.tall_table}
             awk -f {params.awk_script} {output.tall_table} | column -t > {output.long_table}
             """

####rules not used but kept in case they could be helpful later

# rule counting:
        #         input: reads_path+"BAM/{sample}/sortedbyname/{sample}namesort.bam"
        #         output: reads_path+"htseq/strandedis"+strandedness+"/{sample}.txt"
        #         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        #         params: cpu=1,
        #                 gff=gff_file,
        #                 id="{sample}",
        #                 strands="no"
        #         shell:"""
        #               module load python/3.7.3
        #               python -m HTSeq.scripts.count \
        #               -f bam\
        #               -m union \
        #               -r name \
        #               -s {params.strands}\
        #               -t mRNA \
        #               -i Parent \
        #               {input} {params.gff} > {output}
        #               """

####rule combine_matrices_htseq:
        #         input: reads_path+"htseq/"
        #         output: reads_path+"htseq/"+"/matrix.csv"
        #         resources: mem_mb=lambda wildcards,  attempt: (1000*attempt)
        #         params: cpu=1,
        #                 concat_script=rscripts+"htseq_scriptconcat.R"
        #         shell:"""
        #               cd {input}
        #               module load R
        #               Rscript {params.concat_script}
        #               """



# rule joined_tables:
#        input: first=reads_path+"outputsfolder/cmb_orig.txt",
#               sec=reads_path+"outputsfolder/concat.txt",
#               third=reads_path+"outputsfolder/trim.txt",
#               sectolast=reads_path+"outputsfolder/qual.txt",
#               last=reads_path+"outputsfolder/align.txt"
#        output: tall_table=reads_path+"outputsfolder/joined_tables.txt",
#                long_table=reads_path+"outputsfolder/joined_long.txt"
#        resources:mem_mb=lambda wildcards,  attempt: (1000*attempt)
#        params:cpu=4, 
#               awk_script=awkscripts+"tst.awk"
#        shell:"""
#              join --header {input.sectolast} {input.last} | join --header {input.third} - | join --header {input.sec} - | join --header {input.first} - \
#              | column -t > {output.tall_table}
#              awk -f {params.awk_script} {output.tall_table} | column -t > {output.long_table}
#              """